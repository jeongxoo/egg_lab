{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac107e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "490adb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_pyspark():\n",
    "    myConf = pyspark.SparkConf()\n",
    "    spark = pyspark.sql.SparkSession.builder\\\n",
    "        .master(\"local\")\\\n",
    "        .appName(\"myApp\")\\\n",
    "        .config(conf = myConf)\\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95628267",
   "metadata": {},
   "source": [
    "## 데이터 불러오기 & 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d282bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_load(file_name, date, label, spark):\n",
    "    data_df = pd.read_excel(os.path.join(\"data\", file_name), names=[\"data\"])\n",
    "    data_list = list(data_df[\"data\"])\n",
    "    \n",
    "    temp = []\n",
    "    data_temp = []\n",
    "    for i in range(len(data_list)):\n",
    "        temp.append(data_list[i])\n",
    "        if i % 2 != 0:\n",
    "            data_temp.append(temp)\n",
    "            temp = []\n",
    "    \n",
    "    dataRDD = spark.sparkContext.parallelize(data_temp)\n",
    "    \n",
    "    changeData = dataRDD.map(lambda x: (x[0].split(), x[1]))\\\n",
    "                    .map(lambda x: (date + \" \" + x[0][2], x[1]))\\\n",
    "                    .map(lambda x: (x[0].replace(\" \", \" 0\"), x[1]) if len(x[0]) == 15 else (x[0], x[1]))\\\n",
    "                    .sortByKey(lambda x: x[0])\n",
    "    \n",
    "    time_stamp = changeData.map(lambda x: x[0][-5:]).collect()\n",
    "    data_count = changeData.map(lambda x: x[1]).collect()\n",
    "    \n",
    "    return time_stamp, data_count, changeData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09b94f5",
   "metadata": {},
   "source": [
    "## 선형보간전 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "584b4a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def before_linear(data, date, spark):\n",
    "    _data = data.collect()\n",
    "    _f, _e = _data[0][1], _data[-1][1]\n",
    "    _data = dict(_data)\n",
    "    \n",
    "    start_time = date + \" 00:00\"\n",
    "    end_time = date + \" 23:59\"\n",
    "    \n",
    "    _data[start_time] = _f\n",
    "    _data[end_time] = _e\n",
    "\n",
    "    _data = sorted(_data.items(), key = lambda x: x[0])\n",
    "    _data = [k if len(k[0]) != 15 else [] for k in _data]\n",
    "    _data = list(filter(lambda x: len(x[0]) >= 13, _data))\n",
    "    return _data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c529420a",
   "metadata": {},
   "source": [
    "## 선형 보간 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9bb33c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isitRight(step_data):\n",
    "    step_data = sorted(step_data)\n",
    "    for i in range(len(step_data) - 1):\n",
    "        try:\n",
    "            now = datetime.strptime(step_data[i][0], \"%Y-%m-%d %H:%M\")\n",
    "            nxt = datetime.strptime(step_data[i+1][0], \"%Y-%m-%d %H:%M\")\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        now_value = step_data[i][1]\n",
    "        nxt_value = step_data[i+1][1]\n",
    "\n",
    "        new_t = (nxt - now) / 2\n",
    "        # nn분 30초 나오는 애들은 30초를 버리고\n",
    "        # nn분만 살려서 가는 형태로 갑시다\n",
    "        if new_t < timedelta(minutes = 1):\n",
    "            continue\n",
    "        else:\n",
    "            if new_t % timedelta(seconds = 60) != timedelta(seconds = 0):\n",
    "                new_t -= timedelta(seconds = 30)\n",
    "\n",
    "            new_value = (now_value + nxt_value) / 2\n",
    "            new_t = now + new_t\n",
    "            \n",
    "            new_t.strftime(\"%Y-%m-%d %H:%M\")\n",
    "            new_t = str(new_t)\n",
    "            new_t = new_t[:-3]\n",
    "            step_data.append((new_t, new_value))\n",
    "            \n",
    "    return step_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ecdad85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a87eb80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yesItIsRight(data):\n",
    "    startPoint = time.time()     \n",
    "    while len(data) < 1440:\n",
    "        data = isitRight(data)\n",
    "        finishPoint = time.time() \n",
    "        if finishPoint - startPoint >= 5:\n",
    "            print(len(data))\n",
    "            data = list(map(lambda x: (int(x[0][-5:-3]) * 60) + int(x[0][-2:]), data))\n",
    "            for d in data:\n",
    "                if d not in [i for i in range(1440)]:\n",
    "                    print(d)\n",
    "#             data = list(filter(lambda x: x not in [i for i in range(1440)], data))\n",
    "#             print(data)\n",
    "#             ddd = np.array([i for i in range(1440)])\n",
    "#             ccc = np.array(data)\n",
    "            print(len(data))\n",
    "            print(sorted(set([i for i in range(1440)]) - set(data)))\n",
    "            print(len(set(data)))\n",
    "            break\n",
    "    data = sorted(data, key = lambda x: x[0])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570dfcec",
   "metadata": {},
   "source": [
    "## 그래프 그리기 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51ee251e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_graph(data, label, points):\n",
    "    _x = spark.sparkContext.parallelize(data).map(lambda x: x[0]).collect()\n",
    "    _y = spark.sparkContext.parallelize(data).map(lambda x: x[1]).collect()\n",
    "    \n",
    "    plt.plot(_x[:], _y[:], label = label)\n",
    "    plt.rcParams['figure.figsize'] = (20, 4)\n",
    "    plt.rcParams['font.size'] = 10\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8def2030",
   "metadata": {},
   "source": [
    "## 전체 기능 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "57b3d030",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_linear(date, file_name, label, spark):\n",
    "    _t, _d, _a = data_load(file_name, date, label, spark)\n",
    "    _dlinear = before_linear(_a, date, spark)\n",
    "    print()\n",
    "    print(\"#\"*100)\n",
    "    _all = yesItIsRight(_dlinear)\n",
    "    print(len(_all))\n",
    "    return _all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a643af33",
   "metadata": {},
   "source": [
    "## 데이터 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c91e54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(data, file_name):\n",
    "    all_df = pd.DataFrame(data, columns= [\"hearts\", \"steps\"])\n",
    "    all_df.to_csv(os.path.join(\"data\", file_name))\n",
    "    return all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1ab8382",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_data(h_d, s_d):\n",
    "    return [[v[1], s_d[i][1]] for i, v in enumerate(h_d)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46d54f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final(date, _date, spark):\n",
    "    h, s = \"hearts\", \"steps\"\n",
    "    hearts_file_name = _date + \"_\" + h + \".xlsx\"\n",
    "    steps_file_name = _date + \"_\" + s + \".xlsx\"\n",
    "    \n",
    "    heart_data = load_and_linear(date, hearts_file_name, h, spark)\n",
    "    step_data = load_and_linear(date, steps_file_name, s, spark)\n",
    "    \n",
    "    merged = merge_data(heart_data, step_data)\n",
    "    save_file_name = \"all_day_\" + _date + \".csv\"\n",
    "    ans = save_data(merged, save_file_name)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2485664",
   "metadata": {},
   "source": [
    "## 실행파트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45bbd793",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/20 14:26:45 WARN Utils: Your hostname, jeongxoo.local resolves to a loopback address: 127.0.0.1; using 172.20.10.4 instead (on interface en0)\n",
      "22/01/20 14:26:45 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/lib/python3.9/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/01/20 14:26:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = set_pyspark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4d166d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20211118\n"
     ]
    }
   ],
   "source": [
    "date = input()\n",
    "_date = date[2:]\n",
    "date = date[:4] + \"-\" + date[4:6] + \"-\" + date[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b2c196cf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "####################################################################################################\n",
      "1440\n",
      "\n",
      "####################################################################################################\n",
      "1440\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hearts</th>\n",
       "      <th>steps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>72.0</td>\n",
       "      <td>113.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>72.0</td>\n",
       "      <td>113.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>72.0</td>\n",
       "      <td>113.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>72.0</td>\n",
       "      <td>113.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>72.0</td>\n",
       "      <td>113.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1435</th>\n",
       "      <td>86.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1436</th>\n",
       "      <td>86.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1437</th>\n",
       "      <td>86.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1438</th>\n",
       "      <td>86.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439</th>\n",
       "      <td>86.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1440 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      hearts  steps\n",
       "0       72.0  113.0\n",
       "1       72.0  113.0\n",
       "2       72.0  113.0\n",
       "3       72.0  113.0\n",
       "4       72.0  113.0\n",
       "...      ...    ...\n",
       "1435    86.0   40.0\n",
       "1436    86.0   40.0\n",
       "1437    86.0   40.0\n",
       "1438    86.0   40.0\n",
       "1439    86.0   40.0\n",
       "\n",
       "[1440 rows x 2 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final(date, _date, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a94d844",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
